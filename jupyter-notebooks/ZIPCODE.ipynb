{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Benson - Zip Code Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from matplotlib.collections import PatchCollection\n",
    "from matplotlib.patches import Polygon\n",
    "import matplotlib.path as mplPath\n",
    "import re\n",
    "import json, urllib3\n",
    "import shapefile\n",
    "from string import punctuation\n",
    "import pickle\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "#Set options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 50)\n",
    "pd.set_option('display.precision', 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read in NYC Zip Code shapefile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_geojson(url= 'http://catalog.civicdashboards.com/dataset/11fd957a-8885-42ef-aa49-5c879ec93fac/resource/28377e88-8a50-428f-807c-40ba1f09159b/download/nyc-zip-code-tabulation-areas-polygons.geojson'):\n",
    "    response = urllib3.PoolManager().request('GET', url)\n",
    "    with open('NYC_zipcode.txt', 'w') as f:\n",
    "        json.dump(json.loads(response.data), f)\n",
    "        \n",
    "def read_geojson(json_file='/Users/samfunk/ds/metis/metis-one-Benson/NYC_zipcode.txt'):\n",
    "    with open(json_file, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "zip_json = read_geojson()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert shapfile to dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def zip_dictionary(zip_json=zip_json):\n",
    "    zip_dict = {}\n",
    "    for i in zip_json['features']:\n",
    "        zip_dict[int(i['properties']['postalCode'])] = i['geometry']['coordinates'][0]\n",
    "    return zip_dict\n",
    "\n",
    "zip_dict = zip_dictionary()\n",
    "\n",
    "def zipPath(zip_dict=zip_dict):\n",
    "    '''\n",
    "    Copy original zip code dictionary and convert coordinates to Path values for searching\n",
    "    '''\n",
    "    zip_path = zip_dict.copy()\n",
    "    for key, value in zip_path.items():\n",
    "        zip_path[key] = mplPath.Path(value)\n",
    "    return zip_path\n",
    "\n",
    "zip_path = zipPath()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read in MTA station data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_shapefile(mta= '/Users/samfunk/Downloads/MTA_STATIONS/geo_export_34e54a93-5cf1-4a42-a24f-8e2108d208cb.shp'):\n",
    "    return shapefile.Reader(mta)\n",
    "\n",
    "mta_shp = read_shapefile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Format stations and add zip code to each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stations_zip(mta_shp=mta_shp):\n",
    "    '''\n",
    "    Using shapefile package, convert .shp to readable/usable dictionary\n",
    "    '''\n",
    "    stations = []\n",
    "    for feature in mta_shp.shapeRecords():\n",
    "        coords = feature.shape.__geo_interface__['coordinates']\n",
    "        stop = feature.record[2]\n",
    "        line = feature.record[-1:][0]\n",
    "        s = [stop, line, coords]\n",
    "        stations.append(s)\n",
    "    return stations\n",
    "stations = stations_zip()\n",
    "\n",
    "\n",
    "def find(stations, zips):\n",
    "    '''\n",
    "    Loop through each station to figure out which zip code it belongs to\n",
    "    ---\n",
    "    IN: stations dictionary, zip code path dictionary\n",
    "    OUT: updated stations dictionary with appended zip codes\n",
    "    '''\n",
    "    for station_ind, station_stop in enumerate(stations):\n",
    "        coord = station_stop[2]\n",
    "        for zip_key, zip_value in zips.items():\n",
    "            if zip_value.contains_point(coord):\n",
    "                stations[station_ind].append(zip_key)\n",
    "    return stations\n",
    "station_info = find(stations, zip_path)\n",
    "\n",
    "\n",
    "def clean_lines(stations=station_info):\n",
    "    '''\n",
    "    Drop 'Express' from line and give missing zip codes to stations\n",
    "    Return clean dataframe\n",
    "    '''\n",
    "    for ind, val in enumerate(stations):\n",
    "        raw = val[1].split('-')\n",
    "        for route in raw:\n",
    "            if 'Express' in route:\n",
    "                raw.remove(route)\n",
    "        stations[ind][1] = ''.join(raw)\n",
    "\n",
    "    missing = ['231st St','238th St','Bowling Green','Broad Channel','Carroll St','South Ferry','Van Cortlandt Park - 242nd St','Whitehall St']\n",
    "    no_zip = [10463,10463,10004,11693,11231,10004,10471,10004]\n",
    "    i = 0\n",
    "    for ind, j in enumerate(stations):\n",
    "        if missing[i] == j[0]:\n",
    "            stations[ind].append(no_zip[i])\n",
    "            i += 1\n",
    "\n",
    "        if j[0] == '125th St' and j[1] == '456':\n",
    "            stations[ind].append(10035)\n",
    "\n",
    "    #replace_125 = {'125th St': {'line': '456', zipCode=10035}}\n",
    "    return station_info\n",
    "station_info = clean_lines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert to pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "columns = ['station', 'linename', 'coordinates', 'zipcode']\n",
    "df = pd.DataFrame(station_info, columns=columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean station and line names to match MTA data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_stations(name):\n",
    "    '''\n",
    "    Use regex to make multiple changes to station names, specifically remove 'st', 'nd', 'rd', and 'th' from strings.\n",
    "    Hard coding for special edged cases\n",
    "    '''\n",
    "    r = re.compile(r'[{}]+'.format(re.escape(punctuation)))\n",
    "    stop_split = r.split(name.upper())\n",
    "\n",
    "    d = []\n",
    "    for j, y in enumerate(stop_split):\n",
    "        if re.search(r'^\\s', y):\n",
    "            y = re.sub(r'^\\s', '', y)\n",
    "\n",
    "        if re.search(r'\\s$', y):\n",
    "            y = re.sub(r'\\s$', '', y)\n",
    "\n",
    "        if j < 2:\n",
    "            if re.search(r'\\d', y):\n",
    "                if 'BEACH' in y:\n",
    "                    regex = re.search(r'(BEACH\\s\\d+)\\w*(\\s\\w+)', y)\n",
    "                    d.append(regex[1] + regex[2])\n",
    "                elif 'BAY' in y:\n",
    "                    d.append('BAY 50 ST')\n",
    "                elif re.search(r'^[EW]\\s', y):\n",
    "                    regex = re.search(r'([EW]\\s\\d*)\\w*(\\s\\w*)', y)\n",
    "                    d.append(regex[1] + regex[2])\n",
    "                elif re.search(r'(\\d*)\\w*(\\s\\w*)', y):\n",
    "                    regex = re.search(r'\\s?(\\d*)\\w*(\\s\\w*)', y)\n",
    "                    d.append(regex[1] + regex[2])\n",
    "                elif re.search(r'^\\d*\\w', y):\n",
    "                    regex = re.search(r'(\\d+)\\w+', y)\n",
    "                    d.append(regex[1])\n",
    "            elif 'TREMONT' in y:\n",
    "                continue\n",
    "            elif re.search(r'^[A-Z]{2}\\s', y):\n",
    "                regex = re.search(r'(^[A-Z]{2}\\s[A-Z]+)', y)\n",
    "                d.append(regex[1])\n",
    "            elif re.search(r'^BAY\\s', y) or re.search(r'^AVE\\s', y) or re.search(r'^VAN\\s', y) or re.search(r'^NEW\\s', y):\n",
    "                regex = re.search(r'(^[A-Z]+\\s[A-Z]*)', y)\n",
    "                d.append(regex[1])\n",
    "            elif 'GRAND' in y:\n",
    "                d.append(y)\n",
    "            elif re.search(r'\\s', y):\n",
    "                regex = re.search(r'([A-Z]+)\\s([A-Z]{2})', y)\n",
    "                d.append(regex[1] + ' ' + regex[2])\n",
    "            else:\n",
    "                d.append(y)\n",
    "        elif j == 2 and 'ROCKEFELLER' in y:\n",
    "            d.append('ROCK')\n",
    "\n",
    "    return '-'.join(d)\n",
    "df['station'] = df['station'].apply(clean_stations)\n",
    "\n",
    "df['station'] = df['station'].str.replace(' AVE', ' AV')\n",
    "df['station'] = df['station'].str.replace('AVE ', 'AVENUE ')\n",
    "df['station'] = df['station'].str.replace(' PKY', ' PKWY')\n",
    "\n",
    "def clean_lines(name):\n",
    "    if name == '46':\n",
    "        return '6'\n",
    "    else:\n",
    "        return name\n",
    "df['linename'] = df['linename'].apply(clean_lines)\n",
    "\n",
    "df['station_linename'] = df['station'] + ' ' + df['linename']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read in pickle files of cleaned MTA dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These dataframes are sorted by morning and evening rush hours. They have average entires and exits per hour along with the census data containing the median income and estimate for young women with STEM degrees both by zip code. All four of these variables have their own repsective percentile ranks, which will come into play below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_pickle(filename):\n",
    "    with open('/Users/samfunk/ds/metis/metis-project-one/' + filename, 'rb') as pf:\n",
    "        return pickle.load(pf)\n",
    "\n",
    "morning = read_pickle('morning_rush.pkl')\n",
    "evening = read_pickle('evening_rush.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Station Score for morning and evening rush hours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this section, two scores are calculated for each station, one for the morning rush hours and one for the evevning ones. Using the percentiles from the imported dataframe, we will apply weights for each stations' values. The weights used for each parameter may be adjusted given the organization's preferences. For now, we decided to assign both station traffic and young women in STEM a 0.4 and income a 0.2. This allotment allows traffic and women both to be doubly important when compared to income, which tends to crowd out the other variables given a larger weight in a city like New York."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def score(df, weights, time):\n",
    "    '''\n",
    "    IN: df = imported pickle file dataframes, weights = list of weights (must sum to 1), time = dummy string for function conditional\n",
    "    OUT: new df column 'score'\n",
    "    '''\n",
    "    station_weight = weights[0]\n",
    "    income_weight = weights[1]\n",
    "    stem_weight = weights[2]\n",
    "    \n",
    "    df.rename(columns={'Average Entries Per Hour, Percentile': 'entries_rank', 'Average Exits Per Hour, Percentile': 'exits_rank', 'Median income (dollars); Estimate; Households, Percentile': 'median_income_rank', 'Demographic, Percentile': 'stem_rank'})\n",
    "    \n",
    "    if time == 'morning':\n",
    "        df['score'] = station_weight*df['entries_rank'] + income_weight*df['median_income_rank'] + stem_weight*df['stem_rank']\n",
    "    else:\n",
    "        df['score'] = station_weight*df['exits_rank'] + income_weight*df['median_income_rank'] + stem_weight*df['stem_rank']\n",
    "\n",
    "score(morning, [0.4, 0.2, 0.4], 'morning')\n",
    "score(evening, [0.4, 0.2, 0.4], 'evening')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot stations and zip codes together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_morning = morning.sort_values('score', ascending=False)[['station', 'linename', 'coordinates', 'zip code', 'score']].dropna(axis=0)\n",
    "plot_evening = evening.sort_values('score', ascending=False)[['station', 'linename', 'coordinates', 'zip code', 'score']].dropna(axis=0)\n",
    "\n",
    "def plot_map(zip_dict, df):\n",
    "    '''\n",
    "    Plot the zip code boundaries and MTA stations on a single plot, where the size of the station points are dependent on their scores\n",
    "    IN: zip_dict = zip code dictionary from above, df = morning or evening rush hour dataframe\n",
    "    OUT: plot of NYC zip codes and subway stations\n",
    "    '''\n",
    "    fig,ax = plt.subplots()\n",
    "    ax.set_aspect('equal')\n",
    "    \n",
    "    #Plot station coordinates\n",
    "    lon = []\n",
    "    lat = []\n",
    "    s = [10*size for size in df['test score']]\n",
    "    for coords in df['coordinates']:\n",
    "        lon.append(coords[0])\n",
    "        lat.append(coords[1])\n",
    "    plt.scatter(lon, lat, s=s, c='black')\n",
    "    \n",
    "    #Plot zip code boundaries\n",
    "    for key, value in list(zip_dict.items()):\n",
    "        x_lon = np.zeros((len(value),1))\n",
    "        y_lat = np.zeros((len(value),1))\n",
    "        for ip in range(len(value)):\n",
    "            x_lon[ip] = value[ip][0]\n",
    "            y_lat[ip] = value[ip][1]\n",
    "\n",
    "        plt.plot(x_lon, y_lat, linewidth=1)\n",
    "\n",
    "\n",
    "    WEST_LONGITUDE = -74.05\n",
    "    EAST_LONGITUDE = -73.83\n",
    "    NORTH_LATITUDE = 40.92\n",
    "    SOUTH_LATITUDE = 40.65\n",
    "\n",
    "    plt.xlim(WEST_LONGITUDE, EAST_LONGITUDE)\n",
    "    plt.ylim(SOUTH_LATITUDE, NORTH_LATITUDE)\n",
    "    plt.xlabel('Longitude')\n",
    "    plt.ylabel('Latitude')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_map(zip_dict, plot_morning)\n",
    "plt.title('MTA stations scores (Morning Rush)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_map(zip_dict, plot_evening)\n",
    "plt.title('MTA stations scores (Evening Rush)')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
